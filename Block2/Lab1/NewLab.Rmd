---
title: "NewLab"
author: "Tejashree R Mastamardi"
date: "12/21/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
```

Assignment 1: Ensemble Methods

```{r message=FALSE, warning=FALSE}
x1<-runif(100)
x2<-runif(100)

trdata<-cbind(x1,x2)
trlabels<-as.factor(as.numeric(x1<x2))

train <- cbind.data.frame(trdata, 'y'=trlabels)
```


```{r message=FALSE, warning=FALSE}
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)

tedata<-cbind(x1,x2)
y<-as.numeric(x1<x2)
telabels<-as.factor(y)

plot(x1,x2,col=(y+1))

train <- cbind.data.frame(trdata, 'y'=trlabels)
test <- cbind.data.frame(tedata, 'y'=telabels)

Model <- function(){
  MSR_train <- c()
  MSR_test <- c()
  for (i in c(1,10,100)) {
    model <- randomForest(y~., data = train, ntree = i,
                      nodesize = 25, keep.forest = TRUE)
    
    pred <- predict(model, newdata = test)
    con_mat <- table(test$y, pred)
    MSR_test[i] <- 1-sum(diag(con_mat))/sum(con_mat)
    
    print(paste('Number of trees = ', i, 
              "error_Test = ", MSR_test[i]))
  }
  
}

Model()

```

Question 1.1: 

```{r message=FALSE, warning=FALSE}
x1<-runif(100)
x2<-runif(100)

tr_data <- cbind.data.frame(x1, x2)

y <- as.numeric(x1 < x2)
trlabels <- as.factor(y)

train <- cbind.data.frame(tr_data, 'y'=trlabels)

set.seed(1234)

tx1<-runif(1000)
tx2<-runif(1000)

tedata<-cbind(tx1,tx2)
yt<-as.numeric(tx1<tx2)
telabels<-as.factor(yt)

test <- cbind.data.frame(tedata, 'y'=telabels)

iterations <- 1000
subset_size <- 100
```



```{r message=FALSE, warning=FALSE}
#mm = list()

predMat1 = matrix(0, nrow = 1000, ncol = 100)
predMat10 = matrix(0, nrow = 1000, ncol = 100)
predMat100 = matrix(0, nrow = 1000, ncol = 100)

for(i in 1:1000){
  x1 = runif(100)
  x2 = runif(100)
  y = as.factor(as.numeric(x1<x2))
  trainData = data.frame(x1,x2,y)
  mm1 = randomForest(y~., data = trainData, ntree = 1, keep.forest = TRUE, nodesize = 25)
  mm10 = randomForest(y~., data = trainData, ntree = 10, keep.forest = TRUE, nodesize = 25)
  mm100 = randomForest(y~., data = trainData, ntree = 100, keep.forest = TRUE, nodesize = 25)
  predMat1[i,] = mm1$predicted
  predMat10[i,] = mm10$predicted
  predMat100[i,] = mm100$predicted
}

fclass1 = c()
fclass10 = c()
fclass100 = c()
for(i in 1:nrow(predMat1)){
  fclass1[i] = which.max(table(predMat1[i,])) 
  fclass10[i] = which.max(table(predMat10[i,])) 
  fclass100[i] = which.max(table(predMat100[i,])) 
}

for(i in 1:length(fclass1)){
  if(fclass1[i] == 1){
    fclass1[i] = 0
  }
  else{
    fclass1[i] = 1
  }
}

for(i in 1:length(fclass10)){
  if(fclass10[i] == 1){
    fclass10[i] = 0
  }
  else{
    fclass10[i] = 1
  }
}


for(i in 1:length(fclass100)){
  if(fclass100[i] == 1){
    fclass100[i] = 0
  }
  else{
    fclass100[i] = 1
  }
}

confMat1 = table(fclass1, test$y)
mc1 = 1 - sum(diag(confMat1))/sum(confMat1)  

confMat10 = table(fclass10, test$y)
mc10 = 1 - sum(diag(confMat10))/sum(confMat10)  

confMat100 = table(fclass100, test$y)
mc100 = 1 - sum(diag(confMat100))/sum(confMat100)  

print(mc1)
print(mc10)
print(mc100)

```


```{r message=FALSE, warning=FALSE}
#Model1 <- function(){
  MSR_test <- c()
  train_dataset <- list()
  train_dataset = replicate(n = iterations,
                     expr = {train[sample(nrow(train), subset_size),]},
                     simplify = F)
  result <- data.frame(matrix(nrow = 10, ncol = 4))
  #result <- data.frame()
  trees <- c(1,10,100)
  for (k in trees) {
    for (i in 1:10) {
      model <- randomForest(y~., data = train_dataset[i], ntree = k,
                      nodesize = 25, keep.forest = TRUE)
    
      pred <- predict(model, newdata = test)
      con_mat <- table(test$y, pred)
      MSR_test[i] <- 1-sum(diag(con_mat))/sum(con_mat)
      
      
      
      cat("For", k, "trees, Test_error is", MSR_test[i], "\n")
      #print(paste('dataset_number=',i,
       #           'Number of trees = ', k, 
        #          'error_Test = ', MSR_test[i]))
      #result <- rbind(result, c(MSR_test[i]))
      result[i,1] <- i
      result[i,2] <- MSR_test[i]
      result[i,3] <- MSR_test[i]
      result[i,4] <- MSR_test[i]
      }
    
  } 
  result <- as.data.frame(result)
#colnames(result) <- c("Train_dataset_number","1_tree","10_trees","100_trees")

```


Question 1.2: 
```{r message=FALSE, warning=FALSE}
x1<-runif(100)
x2<-runif(100)

tr_data <- cbind.data.frame(x1, x2)

y <- as.numeric(x1 < 0.5)
trlabels <- as.factor(y)

train1 <- cbind.data.frame(tr_data, 'y'=trlabels)

set.seed(1234)

tx1<-runif(1000)
tx2<-runif(1000)

tedata<-cbind(tx1,tx2)
yt<-as.numeric(tx1 < 0.5)
telabels<-as.factor(yt)

test1 <- cbind.data.frame(tedata, 'y'=telabels)
```


```{r message=FALSE, warning=FALSE}
predMat1 = matrix(0, nrow = 1000, ncol = 100)
predMat10 = matrix(0, nrow = 1000, ncol = 100)
predMat100 = matrix(0, nrow = 1000, ncol = 100)

for(i in 1:1000){
  x1 = runif(100)
  x2 = runif(100)
  y = as.factor(as.numeric(x1<0.5))
  trainData = data.frame(x1,x2,y)
  mm1 = randomForest(y~., data = trainData, ntree = 1, keep.forest = TRUE, nodesize = 25)
  mm10 = randomForest(y~., data = trainData, ntree = 10, keep.forest = TRUE, nodesize = 25)
  mm100 = randomForest(y~., data = trainData, ntree = 100, keep.forest = TRUE, nodesize = 25)
  predMat1[i,] = mm1$predicted
  predMat10[i,] = mm10$predicted
  predMat100[i,] = mm100$predicted
}

fclass1 = c()
fclass10 = c()
fclass100 = c()
for(i in 1:nrow(predMat1)){
  fclass1[i] = which.max(table(predMat1[i,])) 
  fclass10[i] = which.max(table(predMat10[i,])) 
  fclass100[i] = which.max(table(predMat100[i,])) 
}

for(i in 1:length(fclass1)){
  if(fclass1[i] == 1){
    fclass1[i] = 0
  }
  else{
    fclass1[i] = 1
  }
}

for(i in 1:length(fclass10)){
  if(fclass10[i] == 1){
    fclass10[i] = 0
  }
  else{
    fclass10[i] = 1
  }
}


for(i in 1:length(fclass100)){
  if(fclass100[i] == 1){
    fclass100[i] = 0
  }
  else{
    fclass100[i] = 1
  }
}

confMat1 = table(fclass1, test1$y)
mc1 = 1 - sum(diag(confMat1))/sum(confMat1)  

confMat10 = table(fclass10, test1$y)
mc10 = 1 - sum(diag(confMat10))/sum(confMat10)  

confMat100 = table(fclass100, test1$y)
mc100 = 1 - sum(diag(confMat100))/sum(confMat100)  

print(mc1)
print(mc10)
print(mc100)

```

Question 1.3:
```{r message=FALSE, warning=FALSE}
set.seed(1234)

tx1<-runif(1000)
tx2<-runif(1000)

tedata<-cbind(tx1,tx2)
yt<-as.numeric(tx1 < 0.5 && tx2 < 0.5 | tx1 > 0.5 && tx2 > 0.5)
telabels<-as.factor(yt)

test2 <- cbind.data.frame(tedata, 'y'=telabels)
```


```{r message=FALSE, warning=FALSE}
predMat1 = matrix(0, nrow = 1000, ncol = 100)
predMat10 = matrix(0, nrow = 1000, ncol = 100)
predMat100 = matrix(0, nrow = 1000, ncol = 100)

for(i in 1:1000){
  x1 = runif(100)
  x2 = runif(100)
  y = as.factor(as.numeric(x1<0.5 & x2<0.5 | x1>0.5 & x2>0.5))
  trainData1 = data.frame(x1,x2,y)
  mm1 = randomForest(y~., data = trainData1, ntree = 1, keep.forest = TRUE, nodesize = 12)
  mm10 = randomForest(y~., data = trainData1, ntree = 10, keep.forest = TRUE, nodesize = 12)
  mm100 = randomForest(y~., data = trainData1, ntree = 100, keep.forest = TRUE, nodesize = 12)
  predMat1[i,] = mm1$predicted
  predMat10[i,] = mm10$predicted
  predMat100[i,] = mm100$predicted
}

fclass1 = c()
fclass10 = c()
fclass100 = c()
for(i in 1:nrow(predMat1)){
  fclass1[i] = which.max(table(predMat1[i,])) 
  fclass10[i] = which.max(table(predMat10[i,])) 
  fclass100[i] = which.max(table(predMat100[i,])) 
}

for(i in 1:length(fclass1)){
  if(fclass1[i] == 1){
    fclass1[i] = 0
  }
  else{
    fclass1[i] = 1
  }
}

for(i in 1:length(fclass10)){
  if(fclass10[i] == 1){
    fclass10[i] = 0
  }
  else{
    fclass10[i] = 1
  }
}


for(i in 1:length(fclass100)){
  if(fclass100[i] == 1){
    fclass100[i] = 0
  }
  else{
    fclass100[i] = 1
  }
}

confMat1 = table(fclass1, test2$y)
mc1 = 1 - sum(diag(confMat1))/sum(confMat1)  

confMat10 = table(fclass10, test2$y)
mc10 = 1 - sum(diag(confMat10))/sum(confMat10)  

confMat100 = table(fclass100, test2$y)
mc100 = 1 - sum(diag(confMat100))/sum(confMat100)  

print(mc1)
print(mc10)
print(mc100)

```

Assignment 2: Mixture Models

EM Algorithm is implemented for a mixture of multivariate Bernoulli Distributions for the components K = 2,3,4. Then the mixtures models are compared. 

Values of Pi, Mu are initialized as true_pi, ture_mu. A matrix $X$ is built using random values of Binomial Distribution. 

```{r message=FALSE, warning=FALSE}
set.seed(1234567890)
max_it <- 100 #max no of EM iterations
min_change <- 0.1 #min change in log likelihood between two consecutive EM iterations
N <- 1000 #no of training points
D <- 10 #no of dimensions
x <- matrix(nrow = N, ncol = D) #training data

true_pi <- vector(length=3) #true mixing coefficients
true_pi <- c(1/3,1/3,1/3)
true_mu <- matrix(nrow = 3, ncol = D) #true conditional distributions
true_mu[1,] = c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,] = c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,] = c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)

plot(true_mu[1,], type = "o", col = "blue", ylim = c(0,1))
points(true_mu[2,], type = "o", col = "red")
points(true_mu[3,], type = "o", col = "green")

#Producing the training data
for (n in 1:N) {
  k <- sample(1:3,1,prob = true_pi)
  for (d in 1:D) {
    x[n,d] <- rbinom(n = 1, size = 1, prob = true_mu[k,d])
  }
}
```


The function for EM Algorithm is written here
```{r, echo=TRUE}
EMfun = function(N,K,x,D, min_change, max_it){
  z = matrix(nrow = N, ncol = K)
  mu = matrix(nrow = K, ncol = D)
  for (k in 1:K) {
    mu[k,] = runif(D, 0.49, 0.51)
  }
  pi = runif(K, 0.49,0.51)
  pi = pi/sum(pi)
  logLik = vector(length = max_it)
  llPrev = 0
  for (it in 1:max_it) {
    for (r in 1:N) {
      for (c in 1:K) {
        z[r,c] = prod(dbinom(x[r,], 1, mu[c,])) * pi[c]
      }
    }
   # E-Step
  logLik[it] = sum(log(rowSums(z)))
    cat("iteration: ", it, "log likelihood: ", logLik[it], "\n")
    flush.console()
    #Stop if the lok likelihood has not changed significantly
    if (abs(llPrev - logLik[it]) < min_change){
      break
    }
    else{
      llPrev = logLik[it]
    }
    # M-Step: ML parameter estimation from the data and fractional component assignments
    
    for (r in 1:N) {
      z[r,] = z[r,]/sum(z[r,])
    }
    
    zcolSum = colSums(z)
    pi = zcolSum/N
    for (i in 1:K) {
      mu[i,] = colSums(x*z[,i])/zcolSum[i]
    }
  }
  out = list(it = it, LogLik = logLik, mu = mu, pi = pi)
  return(out)
}
```


This funtion is used for implementing EM algorithm that will take the number of components as one of the inputs, compute E-step and M-step and returns a list of Estimated Mu and Pi values along with Log-Likelihood values. 

**For K = 2**


```{r, echo=TRUE}

k2Comp = EMfun(N,2,x,D,min_change, max_it)

```

```{r, echo=FALSE}

cat("Pi values for Component K = 2 are\n")
print(k2Comp$pi)
cat("\n\nMU values for Component K = 2 are\n")
print(k2Comp$mu)


```



Here is the plot for Log-Likelihood change upto the Iteration with minimum change.

```{r, echo=FALSE}
plot(1:k2Comp$it, k2Comp$LogLik[1:k2Comp$it], type = "o", lwd = 2,  xlab = "Iterations", 
     ylab = "LogLikelihood", main = "Log Likelihood for K = 2")
```

Here is the comparison plot of mu values we have initialized Vs the estimated mu values. 

```{r, echo=TRUE}
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1), main = "Mu Value Comparison for K = 2", ylab = "True Mu/Estimated Mu")
points(true_mu[2,], type="o", col="blue")
#points(true_mu[3,], type="o", col="blue")
points(k2Comp$mu[1,], type="o", col="red")
points(k2Comp$mu[2,], type="o", col="red")
legend(1,1,legend = c("True MU", "Estimated Mu"), col = c("blue", "red"), lwd=1, cex=0.8)
#points(k2Comp$mu[3,], type="o", col="red")

```


**For K = 3**

```{r, echo=TRUE}
k3Comp = EMfun(N,3,x,D,min_change,max_it)
```



```{r, echo=FALSE}

cat("Pi values for Component K = 3 are\n")
print(k3Comp$pi)
cat("\n\nMU values for Component K = 3 are\n")
print(k3Comp$mu)


```


Here is the plot for Log-Likelihood change upto the Iteration with minimum change.

```{r, echo=TRUE}
plot(1:k3Comp$it, k3Comp$LogLik[1:k3Comp$it], type = "o", lwd = 2,  xlab = "Iterations", 
     ylab = "LogLikelihood", main = "Log Likelihood for K = 3")
```

Here is the comparison plot of mu values we have initialized Vs the estimated mu values.

```{r, echo=TRUE}
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1), main = "Mu Value Comparison for K = 3", ylab = "True Mu/Estimated Mu")
points(true_mu[2,], type="o", col="blue")
points(true_mu[3,], type="o", col="blue")
points(k3Comp$mu[1,], type="o", col="red")
points(k3Comp$mu[2,], type="o", col="red")
points(k3Comp$mu[3,], type="o", col="red")
legend(1,1,legend = c("True MU", "Estimated Mu"), col = c("blue", "red"), lwd=1, cex=0.8)

```


**For K = 4**

```{r, echo=TRUE}
k4Comp = EMfun(N,4,x,D,min_change,max_it)
```


```{r, echo=FALSE}

cat("Pi values for Component K = 4 are\n")
print(k4Comp$pi)
cat("\n\nMU values for Component K = 4 are\n")
print(k4Comp$mu)

```


Here is the plot for Log-Likelihood change upto the Iteration with minimum change.

```{r, echo=TRUE}
plot(1:k4Comp$it,k4Comp$LogLik[1:k4Comp$it], type = "o", lwd = 2,  xlab = "Iterations", 
     ylab = "LogLikelihood", main = "Log Likelihood for K = 4")
```

Here is the comparison plot of mu values we have initialized Vs the estimated mu values.

```{r, echo=TRUE}
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1), main = "Mu Value Comparison for K = 4", ylab = "True Mu/Estimated Mu")
points(true_mu[2,], type="o", col="blue")
points(true_mu[3,], type="o", col="blue")
points(k4Comp$mu[1,], type="o", col="red")
points(k4Comp$mu[2,], type="o", col="red")
points(k4Comp$mu[3,], type="o", col="red")
points(k4Comp$mu[4,], type="o", col="red")
legend(1,1,legend = c("True MU", "Estimated Mu"), col = c("blue", "red"), lwd=1, cex=0.8)

```

Usually, too less or too many components will lead to Underfitting or Overfitting problems. From the above result, it can be observed that when there are two components, there was no minimum change in Likelihood after very few iterations. This could be the case of Underfitting. K = 4 could be a case of Overfitting. 


Assignment 3: High-dimensional methods

Question 3.1: Divide data into train and test sets (70/30) without scaling. Perform Nearest Shrunken Centroid classification of training data in which the threshold is chosen by cross-validation. Provide a centroid plot and interpret it.How many genes were selected by the method? What meaning do positive and negative values have in the centroid plot? Can it happen that all values in the centroid plot are positive for some gene?


```{r message=FALSE, warning=FALSE}
#library(readr)
data=read.table("geneexp.csv", row.names = 1, header =T, sep = ',')

# Dividing data into train and test data (70/30)
n <- dim(data)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.7))
train <- data[id,]
test <- data[-id,]

x <- t(train[,-2086])
y <- train[[2086]]

mydata <- list(x=x,y=as.factor(y),geneid=as.character(1:nrow(x)), genenames=rownames(x))

# Buiding Nearest Shrunken Centroid model
library(pamr)
model <- pamr.train(mydata)

#Choosing threshold by cross validation
modelCV <- pamr.cv(model,mydata)

pamr.plotcv(modelCV)
print(modelCV)

threshold <- modelCV$threshold[which.min(modelCV$error)]

# Fitting model with threshold corresponding to minimum error
modelFinal <- pamr.train(mydata, threshold = threshold)

# Centroid plot
pamr.plotcen(model, mydata, threshold = threshold)

# Number of features selected by method
a=pamr.listgenes(model, mydata, threshold=threshold, genenames=T)

numFea_nsc <- nrow(as.data.frame(a))
numFea_nsc

```

Question 3.2: List the names of the 2 most contributing genes and find their alternative names in Google.
```{r message=FALSE, warning=FALSE}

# 2 most contributing features
cat(paste(colnames(train)[as.numeric(a[1,1])], "alternative name:", "CD74(DHLAG)"), collapse='\n')
cat(paste(colnames(train)[as.numeric(a[2,1])], collapse='\n', "alternative name:", "HLA-DRA(HLA-DRA1)"))

```

```{r message=FALSE, warning=FALSE}
# Test error
testX <- t(test[,-2086])
predY <- pamr.predict(model, newx=testX, type="class",
                      threshold=threshold)

conMat <- table(predY,test$CellType)
mcr_nsr <- 1 - (sum(diag(conMat))/sum(conMat))
mcr_nsr
```

Question 3.3(a):  Compute test error and contributing features for Elastic net with the binomial response and alpha = 0.5 in which penalty is selected by the cross validation 
```{r message=FALSE, warning=FALSE}
library(glmnet)
set.seed(12345)

X <- as.matrix(train[,-2086])
Y <- as.factor(train$CellType)

X_test <- as.matrix(test[,-2086])
Y_test <- as.matrix(test[,2086])

# Fitting elastic model
EN_Model <- cv.glmnet(x=X, y=Y, family="multinomial", 
                      type.measure = "class",
                      alpha = 0.5)
predY_EN <- predict(object = EN_Model, newx = X_test, 
                    s = EN_Model$lambda.min,
                    type = "class", exact = TRUE)
conMat_EN <- table(Y_test,predY_EN)
mcr_EN <- 1 - (sum(diag(conMat_EN))/sum(conMat_EN))
mcr_EN

#cat(paste("number of variables chosen = ",length(coef(EN_Model,s="lambda.min"))-1))

myCoefs <- coef(EN_Model, s="lambda.min")
sel_fea <- (length(which(myCoefs$CD19[,1] != 0)) - 1) +
           (length(which(myCoefs$CD4[,1] != 0)) - 1) +
           (length(which(myCoefs$CD8[,1] != 0)) - 1)

sel_fea
```



```{r message=FALSE, warning=FALSE}

Y <- as.factor(train$CellType)
X <- as.matrix(train[,-2086])

X_test <- as.matrix(test[,-2086])
Y_test <- as.matrix(test[,2086])

library(kernlab)

# Fitting SVM model
svm <- ksvm(X, kernel="vanilladot", scaled=F)
predY_svm <- predict(svm, X_test, type="response")
conMat_svm <- table(Y_test,predY_svm)
mcr_svm <- 1 - (sum(diag(conMat_svm))/sum(conMat_svm))
mcr_svm

# Number of contributing features
svm_coef <- coef(svm, s = "lambda.min")
numFea_svm <- length(svm_coef)


df_misclas <- c(mcr_nsr,mcr_EN,mcr_svm)
numFea <- c(numFea_nsc,sel_fea,numFea_svm)

tab <- rbind(df_misclas,numFea)
colnames(tab) <- c("NSC","Elastic","SVM")
rownames(tab) <- c("error rate", "number of contributing features")
tab

```
 I would prefer Elastic Net model as it's error rate is less compared to the other two models and it also has more number of cinrtibuting features.
 
Question 3.4: 
```{r message=FALSE, warning=FALSE, eval=TRUE}
set.seed(12345)
a <- c()
x <- data[,-2086]
for (i in 1:(length(data)-1)){
  x <- data[,1]
res <- t.test(x, data=data)
a[i] <- res$p.value
}

p_adj <- p.adjust(a, method = "BH", n=length(a))
p_adj_df <- data.frame("feature" = colnames(data[,-2086]), "pvals" = p_adj)
p_adj_df <- p_adj_df[which(p_adj_df[,2] <= 0.05), ]
num_features <- nrow(p_adj_df)
p_adj_df[c(1:39),1]

```

