transition_model <- function(x, y, action, beta){
# Computes the new state after given action is taken. The agent will follow the action
# with probability (1-beta) and slip to the right or left with probability beta/2 each.
#
# Args:
#   x, y: state coordinates.
#   action: which action the agent takes (in {1,2,3,4}).
#   beta: probability of the agent slipping to the side when trying to move.
#   H, W (global variables): environment dimensions.
#
# Returns:
#   The new state after the action has been taken.
delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
final_action <- ((action + delta + 3) %% 4) + 1
foo <- c(x,y) + unlist(action_deltas[final_action])
foo <- pmax(c(1,1),pmin(foo,c(H,W)))
return (foo)
}
q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95,
beta = 0, test = 0){
# Just setting epsilon=0 instead of using the test argument is also OK.
# But in that case the agent acts greedily while still updating the q-table.
cur_position <- start_state
episode_correction <- 0
ite <- 0
repeat{
# Follow policy, execute action, get reward.
action <- EpsilonGreedyPolicy(cur_position[1], cur_position[2], epsilon*(1-test))
new_position <- transition_model(cur_position[1], cur_position[2], action, beta)
reward <- reward_map[new_position[1], new_position[2]]
# Q-table update.
old_q <- q_table[cur_position[1], cur_position[2], action]
correction <- ifelse(reward==0,-1,reward) + gamma*max(q_table[new_position[1], new_position[2], ]) - old_q
q_table[cur_position[1], cur_position[2], action] <<- old_q + alpha*correction*(1-test)
cur_position <- new_position
episode_correction <- episode_correction + correction*(1-test)
if(reward!=0)
# End episode.
return (c(reward-ite,episode_correction))
else
ite <- ite+1
}
}
SARSA <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95,
beta = 0, test = 0){
cur_position <- start_state
cur_action <- EpsilonGreedyPolicy(cur_position[1], cur_position[2], epsilon*(1-test))
episode_correction <- 0
ite <- 0
repeat{
# Follow policy, execute action, get reward.
new_position <- transition_model(cur_position[1], cur_position[2], cur_action, beta)
## get a'' and s''
action_1 = EpsilonGreedyPolicy(new_position[1], new_position[2], epsilon*(1-test))
position_1 = transition_model(new_position[1], new_position[2], action_1, beta)
reward <- reward_map[new_position[1], new_position[2]]
## get r'
reward_1 = reward_map[position_1[1], position_1[2]]
new_action <- EpsilonGreedyPolicy(new_position[1], new_position[2], epsilon*(1-test))
# Q-table update.
old_q <- q_table[cur_position[1], cur_position[2], cur_action]
correction <- ifelse(reward==0,-1,reward) +
gamma*ifelse(reward_1==0,-1,reward_1) +
(gamma^2)*q_table[pos_1[1], pos_1[2], action_1] -
old_q
q_table[cur_position[1], cur_position[2], cur_action] <<- old_q + alpha*correction*(1-test)
cur_position <- new_position
cur_action <- new_action
episode_correction <- episode_correction + correction*(1-test)
## episode ends when r or r' are 10 or -10
if(reward==10 || reward==-10 || reward_1==10 || reward_1==-10)
# End episode.
return (c(reward-ite,episode_correction))
else
ite <- ite+1
}
}
MovingAverage <- function(x, n){
cx <- c(0,cumsum(x))
rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
return (rsum)
}
H <- 3
W <- 6
reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -10
reward_map[1,6] <- 10
# To avoid having to modify vis_environment, I take care of the reward -1 in the functions
# q_learning and SARSA.
q_table <- array(0,dim = c(H,W,4))
rewardqtr <- NULL
for(i in 1:5000){
foo <- q_learning(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 0)
rewardqtr <- c(rewardqtr,foo[1])
}
vis_environment(5000, epsilon = 0.5, gamma = 1, beta = 0)
rewardqte <- NULL
for(i in 1:5000){
foo <- q_learning(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 1)
rewardqte <- c(rewardqte,foo[1])
}
q_table <- array(0,dim = c(H,W,4))
rewardstr <- NULL
for(i in 1:5000){
foo <- SARSA(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 0)
rewardstr <- c(rewardstr,foo[1])
}
# install.packages("ggplot2")
# install.packages("vctrs")
library(ggplot2)
# If you do not see four arrows in line 16, then do the following:
# File/Reopen with Encoding/UTF-8
arrows <- c("↑", "→", "↓", "←")
action_deltas <- list(c(1,0), # up
c(0,1), # right
c(-1,0), # down
c(0,-1)) # left
vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
# Visualize an environment with rewards.
# Q-values for all actions are displayed on the edges of each tile.
# The (greedy) policy for each state is also displayed.
#
# Args:
#   iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
#   reward_map (global variable): a HxW array containing the reward given at each state.
#   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
#   H, W (global variables): environment dimensions.
df <- expand.grid(x=1:H,y=1:W)
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
df$val1 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
df$val2 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
df$val3 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
df$val4 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y)
ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
df$val5 <- as.vector(foo)
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
df$val6 <- as.vector(foo)
print(ggplot(df,aes(x = y,y = x)) +
scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
geom_tile(aes(fill=val6)) +
geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
geom_text(aes(label = val5),size = 10) +
geom_tile(fill = 'transparent', colour = 'black') +
ggtitle(paste("Q-table after ",iterations," iterations\n",
"(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
theme(plot.title = element_text(hjust = 0.5)) +
scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
}
GreedyPolicy <- function(x, y){
# Get a greedy action for state (x,y) from q_table.
#
# Args:
#   x, y: state coordinates.
#   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
#
# Returns:
#   An action, i.e. integer in {1,2,3,4}.
# Your code here.
foo <- which(q_table[x,y,] == max(q_table[x,y,]))
return (ifelse(length(foo)>1,sample(foo, size = 1),foo))
}
EpsilonGreedyPolicy <- function(x, y, epsilon){
# Get an epsilon-greedy action for state (x,y) from q_table.
#
# Args:
#   x, y: state coordinates.
#   epsilon: probability of acting randomly.
#
# Returns:
#   An action, i.e. integer in {1,2,3,4}.
# Your code here.
foo <- sample(0:1,size = 1,prob = c(epsilon,1-epsilon))
return (ifelse(foo == 1,GreedyPolicy(x,y),sample(1:4,size = 1)))
}
transition_model <- function(x, y, action, beta){
# Computes the new state after given action is taken. The agent will follow the action
# with probability (1-beta) and slip to the right or left with probability beta/2 each.
#
# Args:
#   x, y: state coordinates.
#   action: which action the agent takes (in {1,2,3,4}).
#   beta: probability of the agent slipping to the side when trying to move.
#   H, W (global variables): environment dimensions.
#
# Returns:
#   The new state after the action has been taken.
delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
final_action <- ((action + delta + 3) %% 4) + 1
foo <- c(x,y) + unlist(action_deltas[final_action])
foo <- pmax(c(1,1),pmin(foo,c(H,W)))
return (foo)
}
q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95,
beta = 0, test = 0){
# Just setting epsilon=0 instead of using the test argument is also OK.
# But in that case the agent acts greedily while still updating the q-table.
cur_position <- start_state
episode_correction <- 0
ite <- 0
repeat{
# Follow policy, execute action, get reward.
action <- EpsilonGreedyPolicy(cur_position[1], cur_position[2], epsilon*(1-test))
new_position <- transition_model(cur_position[1], cur_position[2], action, beta)
reward <- reward_map[new_position[1], new_position[2]]
# Q-table update.
old_q <- q_table[cur_position[1], cur_position[2], action]
correction <- ifelse(reward==0,-1,reward) + gamma*max(q_table[new_position[1], new_position[2], ]) - old_q
q_table[cur_position[1], cur_position[2], action] <<- old_q + alpha*correction*(1-test)
cur_position <- new_position
episode_correction <- episode_correction + correction*(1-test)
if(reward!=0)
# End episode.
return (c(reward-ite,episode_correction))
else
ite <- ite+1
}
}
SARSA <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95,
beta = 0, test = 0){
cur_position <- start_state
cur_action <- EpsilonGreedyPolicy(cur_position[1], cur_position[2], epsilon*(1-test))
episode_correction <- 0
ite <- 0
repeat{
# Follow policy, execute action, get reward.
new_position <- transition_model(cur_position[1], cur_position[2], cur_action, beta)
## get a'' and s''
action_1 = EpsilonGreedyPolicy(new_position[1], new_position[2], epsilon*(1-test))
position_1 = transition_model(new_position[1], new_position[2], action_1, beta)
reward <- reward_map[new_position[1], new_position[2]]
## get r'
reward_1 = reward_map[position_1[1], position_1[2]]
new_action <- EpsilonGreedyPolicy(new_position[1], new_position[2], epsilon*(1-test))
# Q-table update.
old_q <- q_table[cur_position[1], cur_position[2], cur_action]
correction <- ifelse(reward==0,-1,reward) +
gamma*ifelse(reward_1==0,-1,reward_1) +
(gamma^2)*q_table[position_1[1], position_1[2], action_1] -
old_q
q_table[cur_position[1], cur_position[2], cur_action] <<- old_q + alpha*correction*(1-test)
cur_position <- new_position
cur_action <- new_action
episode_correction <- episode_correction + correction*(1-test)
## episode ends when r or r' are 10 or -10
if(reward==10 || reward==-10 || reward_1==10 || reward_1==-10)
# End episode.
return (c(reward-ite,episode_correction))
else
ite <- ite+1
}
}
MovingAverage <- function(x, n){
cx <- c(0,cumsum(x))
rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
return (rsum)
}
H <- 3
W <- 6
reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -10
reward_map[1,6] <- 10
# To avoid having to modify vis_environment, I take care of the reward -1 in the functions
# q_learning and SARSA.
q_table <- array(0,dim = c(H,W,4))
rewardqtr <- NULL
for(i in 1:5000){
foo <- q_learning(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 0)
rewardqtr <- c(rewardqtr,foo[1])
}
vis_environment(5000, epsilon = 0.5, gamma = 1, beta = 0)
rewardqte <- NULL
for(i in 1:5000){
foo <- q_learning(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 1)
rewardqte <- c(rewardqte,foo[1])
}
q_table <- array(0,dim = c(H,W,4))
rewardstr <- NULL
for(i in 1:5000){
foo <- SARSA(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 0)
rewardstr <- c(rewardstr,foo[1])
}
vis_environment(5000, epsilon = 0.5, gamma = 1, beta = 0)
rewardste <- NULL
for(i in 1:5000){
foo <- SARSA(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 1)
rewardste <- c(rewardste,foo[1])
}
plot(MovingAverage(rewardqtr,100),type = "l",ylim = c(-15,5))
lines(MovingAverage(rewardqte,100),type = "l",lty=2)
lines(MovingAverage(rewardstr,100),type = "l",col = "blue")
lines(MovingAverage(rewardste,100),type = "l",col = "blue",lty=2)
# install.packages("ggplot2")
# install.packages("vctrs")
library(ggplot2)
# If you do not see four arrows in line 16, then do the following:
# File/Reopen with Encoding/UTF-8
arrows <- c("↑", "→", "↓", "←")
action_deltas <- list(c(1,0), # up
c(0,1), # right
c(-1,0), # down
c(0,-1)) # left
vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
# Visualize an environment with rewards.
# Q-values for all actions are displayed on the edges of each tile.
# The (greedy) policy for each state is also displayed.
#
# Args:
#   iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
#   reward_map (global variable): a HxW array containing the reward given at each state.
#   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
#   H, W (global variables): environment dimensions.
df <- expand.grid(x=1:H,y=1:W)
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
df$val1 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
df$val2 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
df$val3 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
df$val4 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y)
ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
df$val5 <- as.vector(foo)
foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
df$val6 <- as.vector(foo)
print(ggplot(df,aes(x = y,y = x)) +
scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
geom_tile(aes(fill=val6)) +
geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
geom_text(aes(label = val5),size = 10) +
geom_tile(fill = 'transparent', colour = 'black') +
ggtitle(paste("Q-table after ",iterations," iterations\n",
"(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
theme(plot.title = element_text(hjust = 0.5)) +
scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
}
GreedyPolicy <- function(x, y){
# Get a greedy action for state (x,y) from q_table.
#
# Args:
#   x, y: state coordinates.
#   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
#
# Returns:
#   An action, i.e. integer in {1,2,3,4}.
# Your code here.
foo <- which(q_table[x,y,] == max(q_table[x,y,]))
return (ifelse(length(foo)>1,sample(foo, size = 1),foo))
}
EpsilonGreedyPolicy <- function(x, y, epsilon){
# Get an epsilon-greedy action for state (x,y) from q_table.
#
# Args:
#   x, y: state coordinates.
#   epsilon: probability of acting randomly.
#
# Returns:
#   An action, i.e. integer in {1,2,3,4}.
# Your code here.
foo <- sample(0:1,size = 1,prob = c(epsilon,1-epsilon))
return (ifelse(foo == 1,GreedyPolicy(x,y),sample(1:4,size = 1)))
}
transition_model <- function(x, y, action, beta){
# Computes the new state after given action is taken. The agent will follow the action
# with probability (1-beta) and slip to the right or left with probability beta/2 each.
#
# Args:
#   x, y: state coordinates.
#   action: which action the agent takes (in {1,2,3,4}).
#   beta: probability of the agent slipping to the side when trying to move.
#   H, W (global variables): environment dimensions.
#
# Returns:
#   The new state after the action has been taken.
delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
final_action <- ((action + delta + 3) %% 4) + 1
foo <- c(x,y) + unlist(action_deltas[final_action])
foo <- pmax(c(1,1),pmin(foo,c(H,W)))
return (foo)
}
q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95,
beta = 0, test = 0){
# Just setting epsilon=0 instead of using the test argument is also OK.
# But in that case the agent acts greedily while still updating the q-table.
cur_position <- start_state
episode_correction <- 0
ite <- 0
repeat{
# Follow policy, execute action, get reward.
action <- EpsilonGreedyPolicy(cur_position[1], cur_position[2], epsilon*(1-test))
new_position <- transition_model(cur_position[1], cur_position[2], action, beta)
reward <- reward_map[new_position[1], new_position[2]]
# Q-table update.
old_q <- q_table[cur_position[1], cur_position[2], action]
correction <- ifelse(reward==0,-1,reward) +
gamma*max(q_table[new_position[1], new_position[2], ]) -
old_q
q_table[cur_position[1], cur_position[2], action] <<- old_q + alpha*correction*(1-test)
cur_position <- new_position
episode_correction <- episode_correction + correction*(1-test)
if(reward!=0)
# End episode.
return (c(reward-ite,episode_correction))
else
ite <- ite+1
}
}
SARSA <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95,
beta = 0, test = 0){
cur_position <- start_state
cur_action <- EpsilonGreedyPolicy(cur_position[1], cur_position[2], epsilon*(1-test))
episode_correction <- 0
ite <- 0
repeat{
# Follow policy, execute action, get reward.
new_position <- transition_model(cur_position[1], cur_position[2], cur_action, beta)
## to get a'' and s''
action_1 = EpsilonGreedyPolicy(new_position[1], new_position[2], epsilon*(1-test))
position_1 = transition_model(new_position[1], new_position[2], action_1, beta)
reward <- reward_map[new_position[1], new_position[2]]
## to get r'
reward_1 = reward_map[position_1[1], position_1[2]]
new_action <- EpsilonGreedyPolicy(new_position[1], new_position[2], epsilon*(1-test))
# Q-table update.
old_q <- q_table[cur_position[1], cur_position[2], cur_action]
correction <- ifelse(reward==0,-1,reward) +
gamma*ifelse(reward_1==0,-1,reward_1) +
(gamma^2)*q_table[position_1[1], position_1[2], action_1] -
old_q
q_table[cur_position[1], cur_position[2], cur_action] <<- old_q + alpha*correction*(1-test)
cur_position <- new_position
cur_action <- new_action
episode_correction <- episode_correction + correction*(1-test)
## episode ends when r or r' are 10 or -10
if(reward==10 || reward==-10 || reward_1==10 || reward_1==-10)
# End episode.
return (c(reward-ite,episode_correction))
else
ite <- ite+1
}
}
MovingAverage <- function(x, n){
cx <- c(0,cumsum(x))
rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
return (rsum)
}
H <- 3
W <- 6
reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -10
reward_map[1,6] <- 10
# To avoid having to modify vis_environment, I took care of the reward -1 in the functions
# q_learning and SARSA.
q_table <- array(0,dim = c(H,W,4))
rewardqtr <- NULL
for(i in 1:5000){
foo <- q_learning(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 0)
rewardqtr <- c(rewardqtr,foo[1])
}
vis_environment(5000, epsilon = 0.5, gamma = 1, beta = 0)
rewardqte <- NULL
for(i in 1:5000){
foo <- q_learning(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 1)
rewardqte <- c(rewardqte,foo[1])
}
q_table <- array(0,dim = c(H,W,4))
rewardstr <- NULL
for(i in 1:5000){
foo <- SARSA(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 0)
rewardstr <- c(rewardstr,foo[1])
}
vis_environment(5000, epsilon = 0.5, gamma = 1, beta = 0)
rewardste <- NULL
for(i in 1:5000){
foo <- SARSA(start_state = c(1,1), epsilon = 0.5, gamma = 1, beta = 0, test = 1)
rewardste <- c(rewardste,foo[1])
}
# plot(MovingAverage(rewardqtr,100),type = "l",ylim = c(-15,5))
# lines(MovingAverage(rewardqte,100),type = "l",lty=2)
# lines(MovingAverage(rewardstr,100),type = "l",col = "blue")
# lines(MovingAverage(rewardste,100),type = "l",col = "blue",lty=2)
data(lizards)
lizards <- data(lizards)
attach(lizards)
data("lizards")
attach(lizards)
data("lizards")
attach(lizards)
View(lizards)
library(bnlearn)
res1 <- hc(lizards)
plot(res1)
ci.test("Height", "Diameter", "Species", test = "x2", data = lizards)
ci.test("Diameter", "Species", "Height", test = "x2", data = lizards)
ci.test("Species","Height", "Diameter", test = "x2", data = lizards)
ci.test("Height", "Diameter", test = "x2", data = lizards)
ci.test("Diameter", "Species", test = "x2", data = lizards)
ci.test("Species","Height", test = "x2", data = lizards)
