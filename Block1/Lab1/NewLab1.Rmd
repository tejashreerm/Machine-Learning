---
title: "NewLab1"
author: "Tejashree R Mastamardi"
date: "11/24/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Assignment 1: Hand written digit recognition

Question 1:
Import the data into R and divide it into training, validation and test sets(50%/25%/25%) by using the partitioning principle specified in the lecture slides
```{r message=FALSE, warning=FALSE}
library(readr)

OptDigits <- read.csv("optdigits.csv", header = FALSE)
#summary(OptDigits)

n = dim(OptDigits)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = OptDigits[id,]

id1 = setdiff(1:n, id)
set.seed(12345)
id2 = sample(id1, floor(n*0.25))
valid = OptDigits[id2,]

id3 = setdiff(id1, id2)
test = OptDigits[id3,]


new_data <- rbind(train, valid)
```

Question 2:
Use training data to fit 30-nearest neighbor classifier with function kknn() and kernel=”rectangular” from package kknn and estimate
• Confusion matrices for the training and test data (use table())
• Misclassification errors for the training and test data
Comment on the quality of predictions for different digits and on the overall prediction quality.
```{r message=FALSE, warning=FALSE}
library(class)
library(kknn)

kknn_classifier <- function(a,b,X){
  model <- kknn(as.factor(V65)~., a, b, k=30, kernel = "rectangular")
  pred = round(model$fitted.values)#round as the classes are 0-9
  tab1 <- table(b$V65, pred)
  misclass <- (1 - sum(diag(tab1))/sum(tab1))*100
  return(list("Conf_matrix"= tab1, "ErrorRate"=misclass))
}

kknn_classifier(train, valid, 30)
kknn_classifier(new_data, test, 30)
```
Question 3:
Find any 2 cases of digit “8” in the training data which were easiest to classify and 3 cases that were hardest to classify (i.e. having highest and lowest probabilities of the correct class). Reshape features for each of these cases as matrix 8x8 and visualize the corresponding digits (by using e.g. heatmap() function with parameters Colv=NA and Rowv=NA) and comment on whether these cases seem to be hard or easy to recognize visually.
```{r message=FALSE, warning=FALSE}
vec = c(1,8,7,6,5)
train1 = train[which(train$V65 == vec),]
valid1 = valid[which(valid$V65 == vec),]
myres = kknn_classifier(train1,valid1,30)
myres$Conf_matrix
heatmap(myres$Conf_matrix, Colv = NA, Rowv = NA, scale="none", xlab="variable", ylab="car", main="heatmap")
```

Question 4:
Fit a K-nearest neighbor classifiers to the training data for different values of k=1,2,…,30 and plot the dependence of the training and validation misclassification errors on the value of K (in the same plot). How does the model complexity
change when K increases and how does it affect the training and validation errors? Report the optimal k according to this plot. Discuss this plot from the perspective of bias-variance tradeoff. Finally, estimate the test error for the model
having the optimal K, compare it with the training and validation errors and make necessary conclusions regarding the model quality.

```{r message=FALSE, warning=FALSE}
#Model_knn <- function(){
  training_error <- rep(0,30)
  validation_error <- rep(0,30)
  for (i in 1:30) {
    model <- kknn(as.factor(V65)~.,train, valid, k = i)
    pred = model$fitted.values
    modeltest <- kknn(as.factor(V65)~.,new_data, test, k = i)
    predtest = modeltest$fitted.values
    tab1 <- table(valid$V65, pred)
    training_error[i] <- (1 - sum(diag(tab1))/sum(tab1))*100
    tab2 <- table(test$V65, predtest)
    validation_error[i] <- (1 - sum(diag(tab2))/sum(tab2))*100
  }
plot(seq(1:30), training_error, col = "red",xlab = "K values", ylab = "MSE")
points(seq(1:30), validation_error, col = "blue")

```

```{r}
which.min(validation_error)
```


Question 5:
Fit K-nearest neighbor classifiers to training data for different values of k = 1,2,…, 30, compute the empirical risk for the validation data as cross-entropy ( when computing log of probabilities add a small constant within log, e.g. 1e-15,
to avoid numerical problems) and plot the dependence of the empirical risk on the value of k. What is the optimal k value here? Why might the cross-entropy be a more suitable choice of the empirical risk function than the misclassification error for this problem?

```{r message=FALSE, warning=FALSE}
model = kknn(as.factor(V65)~.,train, valid, k = 2)
cl = vector()
for (i in 1:(dim(model$prob)[1])) {
  cl[i] = which.max(model$prob[i,])
}  
cm = table(valid$V65, cl)
misc = 1 - sum(diag(cm))/sum(cm)

print(misc*100)
print(training_error[2])

```


Assignment 2: Ridge regression and model selection

```{r message=FALSE, warning=FALSE}
library(readr)
library(glmnet)
parkinsons <- read_csv("parkinsons.csv")
View(parkinsons)
```

Question 1:
Assuming that motor_UPDRS is normally distributed and can be modeled by Ridge regression of the voice characteristics, write down the probabilistic model as a Bayesian model.

Answer in Lecture 1d slide 20

Question 2:
Scale the data and divide it into training and test data (60/40). Due to this, compute all models without intercept in the following steps.

```{r}
s_data <- scale(parkinsons)

n=dim(s_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train=s_data[id,]
test=s_data[-id,]

x_train <- train[,7:22]
y_train <- train[,5]
x_test <-  test[,7:22]
y_test <- test[,5]
```

Question 3(a): 
Loglikelihood function that for a given parameter vector w and dispersion sigma computes the log-likelihood function logP(D|w,sigma) for the model from step 1 for the training data

$$ P(D|w,\sigma^2) = \prod_{i=1}^{n}{\frac {1}{\sqrt{2\pi\sigma^2}}}e^-{\frac{1}{2\sigma^2}}(y_i-w^Tx_i)^2$$
$$({\frac {1}{\sqrt{2\pi\sigma^2}}})^ne^-\sum_{i=1}^{n}{\frac{1}{2\sigma^2}}(y_i-w^Tx_i)^2 $$
$$logP(D|w,\sigma^2)= -n*log({\frac {1}{\sqrt{2\pi\sigma^2}}})- \sum_{i=1}^{n}{\frac{1}{2\sigma^2}}(y_i-w^Tx_i)^2 $$

```{r message=FALSE, warning=FALSE}
x <- s_data[,7:22]#Voice Characteristics
y <- s_data[,5]#motor_UPRDS

LogLikelihood <- function(w, sigma, x, y){
  n <- nrow(x)
  sigma <- abs(sigma)
  w <- as.matrix(w)
  result <- -n*log(1/(sqrt(2*pi*sigma^2))) 
            -sum((1/(2*(sigma^2)))*(y - t(t(w) %*% t(x))^2))
  return(result)
}
```


Question 3(b): 
Ridge function that for given vector w, scalar sigma and scalar lambda uses function from 3a and adds up a Ridge penalty to the minus loglikelihood

$$- LogLikelihood + \lambda_0||w||_2^2 $$
```{r message=FALSE, warning=FALSE}
Ridge <- function(w, sigma, lambda, x, y){
  n <- dim(x)[1]
  p <- dim(x)[2]
  x <- as.matrix(x)
  y <- as.matrix(y)
             
  penalty <- (sum(w^2))*(lambda)
  result <- -LogLikelihood(w,sigma,x,y) + penalty
  return(result)
}
```


Question 3(c): 
Ridge Opt function that depends on scalar lambda, uses function from 3b and function optim() with method="BFGS" to find the optimal w and sigma for the given lambda

```{r message=FALSE, warning=FALSE}
RidgeOpt <- function(lambda, x, y){
  n <- dim(x)[1]
  p <- dim(x)[2]
  x <- as.matrix(x)
  y <- as.matrix(y)
  w <- as.matrix(rnorm(p, mean = 0, sd = 0.01))
  sigma <- runif(1,0.001,1)
  sigma <- abs(sigma)
  
  result <- optim(par=w,x=x,y=y,lambda=lambda,
                  sigma=sigma,fn = Ridge,method = "BFGS",
                  lower = -Inf,upper = Inf, 
                  control = list(fnscale=-1))
  return(result)
}

OptResult100 <- RidgeOpt(lambda=100, x=x_train, y=y_train)
```

Question 3(d): 
DF function that for a given scalar lambda computes the degrees of freedom of the regression model from step 1 based on the training data.
```{r message=FALSE, warning=FALSE}
DF <- function(lambda, x, y){
  x <- as.matrix(x)
  y <- as.matrix(y)
  p <- dim(x)[2]
  
  SX <- x %*% (solve((t(x) %*% x) + (lambda * diag(p)))) %*% t(x)
  df <- sum(diag(SX))
  return(df)
}

df <- DF(lambda=100, x=x_train, y=y_train)
df
```

Question 4:
By using function RidgeOpt, compute optimal w parameters for lambda=1, lambda=100 and lambda=1000. Use the estimated parameters to predict the motor_UPDRS values for training and test data and report the training and test MSE values. Which penalty parameter is most appropriate among the selected ones? Why is MSE a more appropriate measure here than other empirical risk functions?
```{r message=FALSE, warning=FALSE}
W<- function(lambda){ # w parameters
  n<- dim(x)[1]
  p<- dim(x)[2]
  x <- as.matrix(x_train)
  y <- as.matrix(y_train)
  w_ridge<- RidgeOpt(lambda=100,x,y)
  w <- as.matrix(w_ridge$par[1:p])
  return(w)
}

MSE <- function(lambda, x, y){ #MSE value
  n<- dim(x)[1]
  p<- dim(x)[2]
  x <- as.matrix(x)
  y <- as.matrix(y)
  w<- W(lambda)
  msefinal <- (sum((y - t(t(w)%*% t(x)))^2))/n
  return(msefinal)
}

comput_t<- function(lambda){
  x_train<-as.matrix(x_train)
  y_train<-as.matrix(y_train)
  x_test<-as.matrix(x_test)
  y_test<-as.matrix(y_test)
  g<- lambda
  m<- length(g)
  msetrain<-0
  msetest<-0
  
  for(i in 1:m){
    msetrain[i]<- MSE(g[i],x_train,y_train) # MSE for train data
    msetest[i]<- MSE(g[i],x_test,y_test) # MSE for test data
  }
  total<- data.frame(train_mse=msetrain,test_mse=msetest,row.names = g)
  return(total)
}

comput_t(c(1, 100, 1000))
```

Question 5:
Use functions from step 3 to compute AIC(Alkaline Information Criterion) scores for the Ridge models with values lambda = 1, lambda = 100, lambda = 1000 and their corresponding optimal parameters w and sigma computed in step 4. What is the optimal model according to AIC criterio? What is the theoretical advantage of this kind of model selection compared to the holdout model selection done in step 4?

$$  AIC = 2k - 2(LogLikelihood)$$
where k = number of estimated parameters

First, multiply the log likelihood by -2, so that it is positive and smaller values indicate a closer fit.
 Then add 2*k, where k is the number of estimated parameters
```{r message=FALSE, warning=FALSE}
function_AIC <- function(lambda, x, y){
  x <- as.matrix(x)
  y <- as.matrix(y)
  p <- dim(x)[2]
  w <- W(lambda)
  Sigma <- RidgeOpt(lambda, x_train, y_train)
  sigma <- Sigma$par[1]
  
  loglikelihood <- Ridge(w, sigma, lambda, x, y)
  sel_par <- DF(lambda, x, y)
  result <- (2*sel_par) - (2*loglikelihood)
  return(result)
}
```


```{r message=FALSE, warning=FALSE}
compute_AIC <- function(lambda_vector){
  x <- as.matrix(x)
  y <- as.matrix(y)
  n <- length(lambda_vector)
  result_vec <- c()
  
  for (i in 1:n) {
    result_vec[i] <- function_AIC(lambda_vector[i], x, y)
  }
  result <- data.frame("AIC"=result_vec, row.names = lambda_vector)
  return(result)
}

compute_AIC(c(1,100,1000))
```
 

Assignment 3: Linear Regression and LASSO

```{r message=FALSE, warning=FALSE}
library(readr)
tecator <- read_csv("tecator.csv")
View(tecator)
tecator <- tecator[,-1]

n=dim(tecator)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=tecator[id,]
test=tecator[-id,]
```

Question 1:
Assume that Fat can be modeled as a linear regression in which absorbance characteristics (Channels) are used as features. Report the underlying probabilistic model, fit the linear regression to the training data and estimate the training and test errors. Comment on the quality of fit and prediction and therefore on the quality of model.

```{r message=FALSE, warning=FALSE}
model_data <- train[,1:101]

model <- lm(Fat~., data = model_data)
#summary(model)

predTrain <- predict(model, newx = as.matrix(train[,1:100]))
tab1 <- table(train$Fat, predTrain)
Missclass1 <- (1 - sum(diag(tab1))/sum(tab1)) * 100
Missclass1
error_train <- predTrain - train[,101]


predTest <- predict(model, newx = as.matrix(test[,1:100]))
tab2 <- table(test$Fat, predTest)
Missclass <- (1 - sum(diag(tab2))/sum(tab2)) * 100
Missclass
error_test <- predTest - test[,101]

sd(error_train)
sd(error_test)
```


```{r}
library(glmnet)
data <- read.csv(file = 'tecator.csv')
data <- data[,-1]
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
para <- lm(Fat~.,data = train[,-c(102,103)]) #calculate model
y_hat_tr <- predict(para,newx=as.matrix(train[,1:100]),type="response") #predict train value
err_tr <- y_hat_tr-train[,101] #train residuals
y_hat_te <- predict(para,newdata = test[,1:100],type="response") #predict test value
err_te <- y_hat_te-test[,101] #test residuals
sd(err_tr)
sd(err_te)
plot(1:107,err_tr,col="blue",xlim = c(1,108),ylim = c(-50,100),
xlab = "number of sample",ylab = "error for both sets",main = "residuals")
points(1:108,err_te,col="red")
legend(0,80,legend=c("training data","test data"),col=c("blue","red"),lty=1,cex=0.8)
```

Question 2:
Assume now that Fat can be modeled as a LASSO regression in which all Channels are used as features. Report the objective function that should be optimized in this scenario.

Question 3:
Fit the LASSO regression model to the training data. Present a plot illustrating how the regression coefficients depend on the log of penalty factor (log lambda) and interpret this plot. What value of the penalty factor can be chosen if we want to select a model with only three features?

```{r message=FALSE, warning=FALSE}
library(glmnet)
train1 <- train[,-c(102,103)]
covariates <- train1[,-c(101)]
response <- train1$Fat

model <- glmnet(as.matrix(covariates), response, alpha=1, family = "gaussian")

#plot illustrating how the regression coefficients depend on the log of penalty factor
plot(model, xvar="lambda", label = TRUE)

model$lambda[which(model$df == 3)] # pick up those lambdas whose df is 3

```

Question 4:
Present a plot of how degrees of freedom depend on the penalty parameter. Is the observed trend expected?

```{r message=FALSE, warning=FALSE}
model.cv=cv.glmnet(as.matrix(covariates),response, alpha=1,family="gaussian")

model.cv$lambda.min

plot(log(model.cv$glmnet.fit$lambda), model.cv$glmnet.fit$df, type="p", col="blue",
     xlab = "log of lambda", ylab = "df",main = "degree of freedom depends on log of lambda")

```

Question 5:

Repeat step 3 but fit Ridge instead of the LASSO regression and compare the plots from steps 3 and 5. Conclusions?

```{r message=FALSE, warning=FALSE}
model <- glmnet(as.matrix(covariates), response, alpha=0, family = "gaussian")
plot(model, xvar = "lambda", label = TRUE)
```

Question 6: 
Use cross-validation to compute the optimal LASSO model. Present a plot showing the dependence of the CV score on log𝜆𝜆 and comment how the CV score changes with log𝜆𝜆. Report the optimal 𝜆𝜆 and how many variables were chosen in this model. Comment whether the selected 𝜆𝜆 value is statistically significantly better than log𝜆𝜆=−2. Finally, create a scatter plot of the original test versus predicted test values for the model corresponding to optimal lambda and comment whether the model predictions are good.
```{r message=FALSE, warning=FALSE}

cv_lasso <- cv.glmnet(as.matrix(covariates), response, alpha = 1, family = "gaussian")
lasso_coef <- coef.glmnet(cv_lasso,s=cv_lasso$lambda.min)
#lasso_coef[lasso_coef!=0]
selected_coef <- lasso_coef[which(lasso_coef[,1]!=0)]
selected_coef
cat("The number of variables,as lambda take the optimal value, is",length(selected_coef)-1)

plot(cv_lasso)

lasso_model_opt <- glmnet(as.matrix(covariates), response, alpha = 1,
                          family = "gaussian", lambda =cv_lasso$lambda.min)

lasso_model_2 <- glmnet(as.matrix(covariates), response, alpha = 1, 
                        family = "gaussian", lambda = exp(-2))

print(lasso_model_opt)
print(lasso_model_2)

y <- test[,101]
ynew <- predict(lasso_model_opt, newx=as.matrix(test[, 1:100]), type="response")

plot(1:108, ynew, col="red", xlab = "number of sample", ylab = "predicted value and true value") 
points(1:108, y, col="blue")
legend(120, 50, legend=c("test data","predict data"), col=c("blue","red"), pch = 1, cex=0.8)

sum((ynew-mean(y))^2)/sum((y-mean(y))^2)
```

Question 7:
Use the feature values from test data (the portion of test data with Channel columns) and the optimal LASSO model from step 6 to generate new target values. (Hint: use rnorm() and compute 𝜎𝜎 as standard deviation of residuals from train data predictions). Make a scatter plot of original Fat in test data versus newly generated ones. Comment on the quality of the data generation.

```{r message=FALSE, warning=FALSE}

y_hat <- predict(lasso_model_opt, newx=as.matrix(test[, 1:100]), type="response")
y_hat_train <- predict(lasso_model_opt, newx =as.matrix(train[, 1:100]), type="response")
sigma <- sd(y_hat_train-train[,101])
y_simulated <- sapply(y_hat,function(x){rnorm(1,x,sigma)})
plot(test[, 101],y_simulated,main = "test set vs simulated set")
lines(test[, 101],test[, 101],col="blue")
```

